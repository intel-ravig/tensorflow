/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

// See docs in ../ops/math_ops.cc.

// This file uses oneDNN library for acceleration of Batch Matrix-Matrix
// Multiplication (MatMul) operations. We currently register this kernel only
// for oneDNN supported data types (float, bfloat16). The maximum number of
// dimensions (rank) for output tensor is DNNL_MAX_NDIMS = 12 in oneDNN.
// If output tensor rank exceeds 12, we exit with reporting an error message.

#if defined(INTEL_MKL)

#define EIGEN_USE_THREADS

#include "tensorflow/core/framework/register_types.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/framework/tensor_shape.h"
#include "tensorflow/core/framework/type_traits.h"
#include "tensorflow/core/framework/types.h"
#include "tensorflow/core/kernels/fill_functor.h"
#include "tensorflow/core/kernels/matmul_op_impl.h"
#include "tensorflow/core/kernels/mkl/mkl_batch_matmul_helper.h"
#include "tensorflow/core/kernels/mkl/mkl_matmul_ops_common.h"
#include "tensorflow/core/platform/errors.h"
#include "tensorflow/core/platform/types.h"
#include "tensorflow/core/util/matmul_bcast.h"
#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"

namespace tensorflow {

typedef Eigen::ThreadPoolDevice CPUDevice;

//  The third parameter v2_bcast is set to true if we are using V2 otherwise
//  we set it to false.
template <typename Device, typename Tlhs, typename Trhs, typename Toutput,
          bool v2_bcast>
class BatchMatMulMkl : public OpKernel {
 public:
  explicit BatchMatMulMkl(OpKernelConstruction* context) : OpKernel(context) {
    OP_REQUIRES_OK(context, context->GetAttr("adj_x", &adj_x_));
    OP_REQUIRES_OK(context, context->GetAttr("adj_y", &adj_y_));
  }

  virtual ~BatchMatMulMkl() {}

  void Compute(OpKernelContext* ctx) override {
    const Tensor& lhs = ctx->input(0);
    const Tensor& rhs = ctx->input(1);

    if (!v2_bcast) {
      // Using V1, so check to make sure lhs and rhs dimensions are correct and
      // no broadcasting is needed.
      OP_REQUIRES(ctx, lhs.dims() == rhs.dims(),
                  errors::InvalidArgument("lhs and rhs has different ndims: ",
                                          lhs.shape().DebugString(), " vs. ",
                                          rhs.shape().DebugString()));
      const int ndims = lhs.dims();
      OP_REQUIRES(
          ctx, ndims >= 2,
          errors::InvalidArgument("lhs and rhs ndims must be >= 2: ", ndims));
      for (int i = 0; i < ndims - 2; ++i) {
        OP_REQUIRES(ctx, lhs.dim_size(i) == rhs.dim_size(i),
                    errors::InvalidArgument(
                        "lhs.dim(", i, ") and rhs.dim(", i,
                        ") must be the same: ", lhs.shape().DebugString(),
                        " vs ", rhs.shape().DebugString()));
      }
    } else {
      OP_REQUIRES(
          ctx, lhs.dims() >= 2,
          errors::InvalidArgument("In[0] ndims must be >= 2: ", lhs.dims()));
      OP_REQUIRES(
          ctx, rhs.dims() >= 2,
          errors::InvalidArgument("In[1] ndims must be >= 2: ", rhs.dims()));
    }

    // lhs and rhs can have different dimensions
    const auto ndims_lhs = lhs.dims();
    const auto ndims_rhs = rhs.dims();

    // Get broadcast info
    MatMulBCast bcast(lhs.shape().dim_sizes(), rhs.shape().dim_sizes());
    OP_REQUIRES(
        ctx, bcast.IsValid(),
        errors::InvalidArgument(
            "In[0] and In[1] must have compatible batch dimensions: ",
            lhs.shape().DebugString(), " vs. ", rhs.shape().DebugString()));

    TensorShape out_shape = bcast.output_batch_shape();

    auto lhs_rows = lhs.dim_size(ndims_lhs - 2);
    auto lhs_cols = lhs.dim_size(ndims_lhs - 1);
    auto rhs_rows = rhs.dim_size(ndims_rhs - 2);
    auto rhs_cols = rhs.dim_size(ndims_rhs - 1);

    if (adj_x_) std::swap(lhs_rows, lhs_cols);
    if (adj_y_) std::swap(rhs_rows, rhs_cols);
    OP_REQUIRES(ctx, lhs_cols == rhs_rows,
                errors::InvalidArgument(
                    "lhs mismatch rhs shape: ", lhs_cols, " vs. ", rhs_rows,
                    ": ", lhs.shape().DebugString(), " ",
                    rhs.shape().DebugString(), " ", adj_x_, " ", adj_y_));

    out_shape.AddDim(lhs_rows);
    out_shape.AddDim(rhs_cols);
    // The maximum number of DNNL tensor dimensions is DNNL_MAX_NDIMS = 12.
    OP_REQUIRES(
        ctx, out_shape.dims() <= DNNL_MAX_NDIMS,
        errors::InvalidArgument(
            "Rank of output tensor must be <= 12, but is ", out_shape.dims(),
            ". Current implementation supports upto rank 12 tensors."));

    Tensor* out = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));
    if (out->NumElements() == 0) {
      return;
    }
    if (lhs.NumElements() == 0 || rhs.NumElements() == 0) {
      functor::SetZeroFunctor<Device, Toutput> f;
      f(ctx->eigen_device<Device>(), out->flat<Toutput>());
      return;
    }

    // Compute parameters for DNNL matmul primitive.
    MklBatchMatMulHelper bmm;
    auto params = bmm.CreateMatMulParams(lhs.shape(), rhs.shape(), out_shape,
                                         adj_x_, adj_y_);

#ifdef DNNL_AARCH64_USE_ACL
    // ACL does not support reuse of primitives with different data.
    // For matmul, the previous approach (PR #47775) of using Tensor addresses
    // does not work, as the addresses are re-used in matmul with different data
    // The counter  ensure we still benefit from caching via SetMklMatmul().
    params->aarch64_counter =
        MklMatMulPrimitiveFactory<float, Tlhs, Trhs,
                                  Toutput>::IncrementCounter();
#endif
    this->ExtendMklMatMulParams(ctx, *params);

    // Create or retrieve matmul primitive from cache.
    MklMatMulPrimitive<Tlhs, Trhs, Toutput>* matmul_prim =
        MklMatMulPrimitiveFactory<float, Tlhs, Trhs, Toutput>::Get(
            *params, false /* value for do_not_cache */);

    UserScratchPad<unsigned char> scratch_pad;
    scratch_pad.AllocateSPTensor(matmul_prim, ctx);
    // Execute matmul primitive.
    std::shared_ptr<stream> cpu_stream;
    MklDnnThreadPool eigen_tp(ctx);
    cpu_stream.reset(CreateStream(&eigen_tp, matmul_prim->GetEngine()));
    if (this->fusion_data_.size() > 0) {
      matmul_prim->Execute(cpu_stream, lhs.flat<Tlhs>().data(),
                           rhs.flat<Trhs>().data(), out->flat<Toutput>().data(),
                           scratch_pad.Get(), this->fusion_data_);
    } else {
      matmul_prim->Execute(cpu_stream, lhs.flat<Tlhs>().data(),
                           rhs.flat<Trhs>().data(), out->flat<Toutput>().data(),
                           scratch_pad.Get());
    }
  }

 protected:
  virtual void ExtendMklMatMulParams(OpKernelContext* ctx,
                                     MklMatMulParams& params) {}
  std::vector<void*> fusion_data_;

 private:
  bool adj_x_;
  bool adj_y_;
};

// OneDNN uses post-ops to implement different kind of fusions. The category of
// each individual post-op can be inferred from the fused_ops attribute. The
// following enum is used to identify list of required post-ops.
namespace {

enum class FusedComputationType {
  kUndefined,
  kMul,
  kAdd,
  kMulAdd,
  kDequantize,
  kMul_Dequantize,
  kAdd_Dequantize,
  kMulAdd_Dequantize,
};

struct FusedComputationPattern {
  FusedComputationType fused_computation;
  std::vector<string> fused_ops;
};

}  // namespace
enum class PostOpKind { kNone, kOutputScale, kMul, kAdd };

// FusedBatchMatMul has additional inputs, currently forcing all the operands
// of fusion to have same type `U`.
template <typename Device, typename Tlhs, typename Trhs, typename Toutput,
          /*type of additional tensors*/ typename U, bool v2_bcast>
class FusedBatchMatMulMkl
    : public BatchMatMulMkl<Device, Tlhs, Trhs, Toutput, v2_bcast> {
 public:
  explicit FusedBatchMatMulMkl(OpKernelConstruction* context)
      : BatchMatMulMkl<Device, Tlhs, Trhs, Toutput, v2_bcast>(context) {
    InitializeFusion(context);
  }

  virtual ~FusedBatchMatMulMkl() {}

 protected:
  struct PostOpInfo {
    PostOpKind post_op_kind;
    int input_idx = -1;  // Operand tensor index if needed by a post-op.
  };

  std::vector<PostOpInfo> post_op_info_list_;

  // This function is called from constructor.
  void InitializeFusion(OpKernelConstruction* context) {
    std::vector<string> fused_ops;
    OP_REQUIRES_OK(context, context->GetAttr("fused_ops", &fused_ops));
    OP_REQUIRES(context, !fused_ops.empty(),
                errors::InvalidArgument(
                    "Fused BatchMatMul must have at least one fused op."));

    using FCT = FusedComputationType;
    // TODO(intel-tf): Add more patterns when implemented.
    std::vector<FusedComputationPattern> patterns{
        {FCT::kMul, {"Mul"}},
        {FCT::kAdd, {"Add"}},
        {FCT::kMulAdd, {"Mul", "Add"}},
    };
    FusedComputationType fused_computation = FusedComputationType::kUndefined;
    for (const auto& pattern : patterns) {
      if (fused_ops == pattern.fused_ops) {
        fused_computation = pattern.fused_computation;
        break;
      }
    }

    // Configure oneDNN post-ops
    switch (fused_computation) {
      case FCT::kMul:
        post_op_info_list_ = {{PostOpKind::kMul, 2}};
        break;
      case FCT::kAdd:
        post_op_info_list_ = {{PostOpKind::kAdd, 2}};
        break;
      case FCT::kMulAdd:
        post_op_info_list_ = {{PostOpKind::kMul, 2}, {PostOpKind::kAdd, 3}};
        break;
      default:
        OP_REQUIRES_OK(
            context, errors::Unimplemented("Fusion is not implemented: [",
                                           absl::StrJoin(fused_ops, ","), "]"));
    }

    int num_args = 0;
    OP_REQUIRES_OK(context, context->GetAttr("num_args", &num_args));
    this->fusion_data_.resize(num_args);
  }

  virtual void ExtendMklMatMulParams(OpKernelContext* ctx,
                                     MklMatMulParams& params) {
    int idx = 0;
    for (const auto& post_op_info : this->post_op_info_list_) {
      switch (post_op_info.post_op_kind) {
        case PostOpKind::kMul: {
          const Tensor& multiplicand_tensor =
              ctx->input(post_op_info.input_idx);
          // TODO(intel-tf): Relax restriction when oneDNN is performant for
          // arbitrary shapes.
          bool is_supported = multiplicand_tensor.NumElements() == 1 &&
                              params.c_dims.size() == 4;
          OP_REQUIRES(ctx, is_supported,
                      errors::Unimplemented(
                          "Unimplemented multiplicand shape for Mul fusion: ",
                          multiplicand_tensor.shape().DebugString()));
          auto format_tag = memory::format_tag::abcd;
          memory::data_type data_type = MklDnnType<U>();
          memory::dims mul_dims(params.c_dims.size(), 1);
          params.post_op_params.push_back(
              {"mul", {}, mul_dims, data_type, format_tag});
          void* multiplicand_data = static_cast<void*>(
              const_cast<U*>(multiplicand_tensor.flat<U>().data()));
          this->fusion_data_[idx++] = multiplicand_data;
        } break;
        case PostOpKind::kAdd: {
          const Tensor& addend_tensor = ctx->input(post_op_info.input_idx);
          // TODO(intel-tf): Relax restriction when oneDNN is performant for
          // arbitrary shapes.
          bool is_supported = params.c_dims.size() == 4 &&
                              addend_tensor.dims() == params.c_dims.size();
          OP_REQUIRES(ctx, is_supported,
                      errors::Unimplemented(
                          "Unimplemented addend shape for Add fusion: ",
                          addend_tensor.shape().DebugString()));
          auto format_tag = memory::format_tag::abcd;
          memory::data_type data_type = MklDnnType<U>();
          memory::dims addend_dims = TFShapeToMklDnnDims(addend_tensor.shape());
          params.post_op_params.push_back(
              {"add", {}, addend_dims, data_type, format_tag});
          void* addend_data = static_cast<void*>(
              const_cast<U*>(addend_tensor.flat<U>().data()));
          this->fusion_data_[idx++] = addend_data;
        } break;
        default:
          OP_REQUIRES_OK(ctx,
                         errors::Unimplemented("Unsupported post-op-kind."));
      }
    }
  }
};

template <typename Device, typename Tlhs, typename Trhs, typename Toutput,
          typename U>
class QuantizedBatchMatMulOp
    : public BatchMatMulMkl<Device, Tlhs, Trhs, Toutput, /*v2_bcast*/ true> {
 public:
  explicit QuantizedBatchMatMulOp(OpKernelConstruction* context)
      : BatchMatMulMkl<Device, Tlhs, Trhs, Toutput, true>(context) {
    InitializeFusion(context);
  }

  virtual ~QuantizedBatchMatMulOp() {}

 protected:
  struct PostOpInfo {
    PostOpKind post_op_kind;
    struct OperandInfo {
      int idx = -1;  // Operand tensor index if needed by a post-op.
      // Indices of min and max value tensors, if the operand is quantized.
      std::vector<int> min_max_indices;
    } operand_info;
    // Indices of output min and max value tensors. It is used when requantize
    // is fused.
    std::vector<int> min_max_indices;
  };

  std::vector<PostOpInfo> post_op_info_list_;

  int num_operands_;  // Number of regular operands without minmax tensors.

  void InitializeFusion(OpKernelConstruction* context) {
    // Currently, tensor quantized with only SCALED mode is supported.
    string input_quant_mode;
    OP_REQUIRES_OK(context,
                   context->GetAttr("input_quant_mode", &input_quant_mode));
    OP_REQUIRES(context, input_quant_mode == "SCALED",
                errors::Unimplemented(
                    "Input tensors are not quantized with SCALED mode."));

    std::vector<string> fused_ops;
    OP_REQUIRES_OK(context, context->GetAttr("fused_ops", &fused_ops));
    if (fused_ops.empty())
      OP_REQUIRES_OK(context,
                     errors::InvalidArgument(
                         "Fused BatchMatMul must have at least one fused op."));

    using FCT = FusedComputationType;
    // TODO(intel-tf): Add more patterns when implemented.
    std::vector<FusedComputationPattern> patterns{
        {FCT::kDequantize, {"Dequantize"}},
        {FCT::kMul_Dequantize, {"Mul", "Dequantize"}},
        {FCT::kAdd_Dequantize, {"Add", "Dequantize"}},
        {FCT::kMulAdd_Dequantize, {"Mul", "Add", "Dequantize"}},
    };

    FusedComputationType fused_computation = FusedComputationType::kUndefined;
    for (const auto& pattern : patterns) {
      if (fused_ops == pattern.fused_ops) {
        fused_computation = pattern.fused_computation;
        break;
      }
    }

    // Configure oneDNN post ops
    switch (fused_computation) {
      case FCT::kDequantize: {
        num_operands_ = 2;
        post_op_info_list_ = {{PostOpKind::kOutputScale}};
      } break;
      case FCT::kMul_Dequantize: {
        num_operands_ = 3;
        post_op_info_list_ = {{PostOpKind::kOutputScale},
                              {PostOpKind::kMul, {2}}};
        this->fusion_data_.resize(1);
      } break;
      case FCT::kAdd_Dequantize: {
        num_operands_ = 3;
        post_op_info_list_ = {{PostOpKind::kOutputScale},
                              {PostOpKind::kAdd, {2}}};
        this->fusion_data_.resize(1);
      } break;
      case FCT::kMulAdd_Dequantize: {
        num_operands_ = 4;
        post_op_info_list_ = {{PostOpKind::kOutputScale},
                              {PostOpKind::kMul, {2}},
                              {PostOpKind::kAdd, {3}}};
        this->fusion_data_.resize(2);
      } break;
      default:
        OP_REQUIRES(context, false,
                    errors::Unimplemented("Fusion is not implemented: [",
                                          absl::StrJoin(fused_ops, ","), "]"));
    }
  }

  void ExtendMklMatMulParams(OpKernelContext* ctx,
                             MklMatMulParams& params) override {
    int idx = 0;
    for (const auto& post_op_info : post_op_info_list_) {
      switch (post_op_info.post_op_kind) {
        case PostOpKind::kMul: {
          const Tensor& multiplicand_tensor =
              ctx->input(post_op_info.operand_info.idx);
          // TODO(intel-tf): Relax restriction when oneDNN is performant for
          // arbitrary shapes.
          bool is_supported = multiplicand_tensor.NumElements() == 1 &&
                              params.c_dims.size() == 4;
          OP_REQUIRES(ctx, is_supported,
                      errors::Unimplemented("Unimplemented"));
          auto format_tag = memory::format_tag::abcd;
          memory::data_type data_type = MklDnnType<U>();
          memory::dims mul_dims(params.c_dims.size(), 1);
          params.post_op_params.push_back(
              {"mul", {}, mul_dims, data_type, format_tag});
          void* multiplicand_data = static_cast<void*>(
              const_cast<U*>(multiplicand_tensor.flat<U>().data()));
          this->fusion_data_[idx++] = multiplicand_data;
        } break;
        case PostOpKind::kAdd: {
          const Tensor& addend_tensor =
              ctx->input(post_op_info.operand_info.idx);
          // TODO(intel-tf): Relax restriction when oneDNN is performant for
          // arbitrary shapes.
          bool is_supported = params.c_dims.size() == 4 &&
                              addend_tensor.dims() == params.c_dims.size();
          OP_REQUIRES(ctx, is_supported,
                      errors::Unimplemented("Unimplemented."));
          auto format_tag = memory::format_tag::abcd;
          memory::data_type data_type = MklDnnType<U>();
          memory::dims addend_dims = TFShapeToMklDnnDims(addend_tensor.shape());
          params.post_op_params.push_back(
              {"add", {}, addend_dims, data_type, format_tag});
          void* addend_data = static_cast<void*>(
              const_cast<U*>(addend_tensor.flat<U>().data()));
          this->fusion_data_[idx++] = addend_data;
        } break;
        case PostOpKind::kOutputScale: {
          // Minmax tensors follows regular operands.
          const int kIdxLhsMin = num_operands_;
          const int kIdxLhsMax = num_operands_ + 1;
          const int kIdxRhsMin = num_operands_ + 2;
          const int kIdxRhsMax = num_operands_ + 3;

          const Tensor& lhs_min_tensor = ctx->input(kIdxLhsMin);
          const Tensor& lhs_max_tensor = ctx->input(kIdxLhsMax);
          const Tensor& rhs_min_tensor = ctx->input(kIdxRhsMin);
          const Tensor& rhs_max_tensor = ctx->input(kIdxRhsMax);
          // Currently, only per tensor quantization supported.
          OP_REQUIRES(ctx,
                      lhs_min_tensor.NumElements() == 1 &&
                          lhs_max_tensor.NumElements() == 1 &&
                          rhs_min_tensor.NumElements() == 1 &&
                          rhs_max_tensor.NumElements() == 1,
                      errors::Unimplemented(
                          "Only supported is per-tensor quantization."));

          const float min_lhs = ctx->input(kIdxLhsMin).flat<float>()(0);
          const float max_lhs = ctx->input(kIdxLhsMax).flat<float>()(0);
          const float min_rhs = ctx->input(kIdxRhsMin).flat<float>()(0);
          const float max_rhs = ctx->input(kIdxRhsMax).flat<float>()(0);

          const float range_lhs =
              std::max(std::abs(min_lhs), std::abs(max_lhs));
          const float range_rhs =
              std::max(std::abs(min_rhs), std::abs(max_rhs));
          const float max_int8_lhs =
              (std::is_same<Tlhs, quint8>::value) ? 255.0f : 127.0f;
          const float max_int8_rhs =
              (std::is_same<Trhs, quint8>::value) ? 255.0f : 127.0f;
          float scale_output =
              range_lhs * range_rhs / (max_int8_lhs * max_int8_rhs);
          params.post_op_params.push_back({"output_scale", {scale_output}});
        } break;
        default:
          OP_REQUIRES(ctx, false,
                      errors::Unimplemented("Unsupported post-op-kind."));
      }
    }
  }
};

#define REGISTER_BATCH_MATMUL_MKL(TYPE)                                       \
  REGISTER_KERNEL_BUILDER(Name("_MklBatchMatMul")                             \
                              .Device(DEVICE_CPU)                             \
                              .TypeConstraint<TYPE>("T")                      \
                              .Label(mkl_op_registry::kMklNameChangeOpLabel), \
                          BatchMatMulMkl<CPUDevice, TYPE, TYPE, TYPE, false>)

#define REGISTER_BATCH_MATMUL_MKL_V2(TYPE)                                    \
  REGISTER_KERNEL_BUILDER(Name("_MklBatchMatMulV2")                           \
                              .Device(DEVICE_CPU)                             \
                              .TypeConstraint<TYPE>("T")                      \
                              .Label(mkl_op_registry::kMklNameChangeOpLabel), \
                          BatchMatMulMkl<CPUDevice, TYPE, TYPE, TYPE, true>)

#define REGISTER_FUSED_BATCH_MATMUL_MKL(TYPE) \
  REGISTER_KERNEL_BUILDER(                    \
      Name("_MklFusedBatchMatMulV2")          \
          .Device(DEVICE_CPU)                 \
          .TypeConstraint<TYPE>("T"),         \
      FusedBatchMatMulMkl<CPUDevice, TYPE, TYPE, TYPE, TYPE, true>)

TF_CALL_float(REGISTER_BATCH_MATMUL_MKL);
TF_CALL_float(REGISTER_BATCH_MATMUL_MKL_V2);
TF_CALL_float(REGISTER_FUSED_BATCH_MATMUL_MKL);
TF_CALL_bfloat16(REGISTER_BATCH_MATMUL_MKL);
TF_CALL_bfloat16(REGISTER_BATCH_MATMUL_MKL_V2);
TF_CALL_bfloat16(REGISTER_FUSED_BATCH_MATMUL_MKL);

#define REGISTER_CPU(T)                \
  REGISTER_KERNEL_BUILDER(             \
      Name("_QuantizedBatchMatMul")    \
          .Device(DEVICE_CPU)          \
          .TypeConstraint<qint8>("T1") \
          .TypeConstraint<qint8>("T2") \
          .TypeConstraint<T>("U")      \
          .TypeConstraint<T>("Tout"),  \
      QuantizedBatchMatMulOp<CPUDevice, qint8, qint8, T, T>);

TF_CALL_float(REGISTER_CPU);
TF_CALL_bfloat16(REGISTER_CPU);

}  // end namespace tensorflow
#endif
